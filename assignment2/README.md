    Logs :



    WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

    WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.

    WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
    Instructions for updating:
    Use tf.where in 2.0, which has the same broadcast rule as np.where
    WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.

    WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.

    Train on 60000 samples, validate on 10000 samples
    Epoch 1/20

    Epoch 00001: LearningRateScheduler setting learning rate to 0.003.
    60000/60000 [==============================] - 15s 257us/step - loss: 0.5277 - acc: 0.8512 - val_loss: 0.0891 - val_acc: 0.9830
    Epoch 2/20

    Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.
    60000/60000 [==============================] - 10s 164us/step - loss: 0.2494 - acc: 0.9263 - val_loss: 0.0660 - val_acc: 0.9864
    Epoch 3/20

    Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.
    60000/60000 [==============================] - 10s 164us/step - loss: 0.1988 - acc: 0.9403 - val_loss: 0.0529 - val_acc: 0.9890
    Epoch 4/20

    Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.
    60000/60000 [==============================] - 10s 163us/step - loss: 0.1730 - acc: 0.9452 - val_loss: 0.0371 - val_acc: 0.9919
    Epoch 5/20

    Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.
    60000/60000 [==============================] - 10s 163us/step - loss: 0.1543 - acc: 0.9477 - val_loss: 0.0350 - val_acc: 0.9918
    Epoch 6/20

    Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.
    60000/60000 [==============================] - 10s 164us/step - loss: 0.1393 - acc: 0.9504 - val_loss: 0.0382 - val_acc: 0.9903
    Epoch 7/20

    Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.
    60000/60000 [==============================] - 10s 163us/step - loss: 0.1327 - acc: 0.9510 - val_loss: 0.0299 - val_acc: 0.9924
    Epoch 8/20

    Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.
    60000/60000 [==============================] - 10s 162us/step - loss: 0.1238 - acc: 0.9542 - val_loss: 0.0278 - val_acc: 0.9924
    Epoch 9/20

    Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.
    60000/60000 [==============================] - 10s 162us/step - loss: 0.1184 - acc: 0.9554 - val_loss: 0.0272 - val_acc: 0.9924
    Epoch 10/20

    Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.
    60000/60000 [==============================] - 10s 163us/step - loss: 0.1127 - acc: 0.9554 - val_loss: 0.0284 - val_acc: 0.9921
    Epoch 11/20

    Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.
    60000/60000 [==============================] - 10s 162us/step - loss: 0.1141 - acc: 0.9538 - val_loss: 0.0255 - val_acc: 0.9930
    Epoch 12/20

    Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.
    60000/60000 [==============================] - 10s 167us/step - loss: 0.1092 - acc: 0.9559 - val_loss: 0.0223 - val_acc: 0.9935
    Epoch 13/20

    Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.
    60000/60000 [==============================] - 10s 163us/step - loss: 0.1055 - acc: 0.9561 - val_loss: 0.0229 - val_acc: 0.9937
    Epoch 14/20

    Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.
    60000/60000 [==============================] - 10s 161us/step - loss: 0.1028 - acc: 0.9565 - val_loss: 0.0248 - val_acc: 0.9932
    Epoch 15/20

    Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.
    60000/60000 [==============================] - 10s 163us/step - loss: 0.1045 - acc: 0.9550 - val_loss: 0.0234 - val_acc: 0.9926
    Epoch 16/20

    Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.
    60000/60000 [==============================] - 10s 162us/step - loss: 0.0986 - acc: 0.9579 - val_loss: 0.0242 - val_acc: 0.9929
    Epoch 17/20

    Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.
    60000/60000 [==============================] - 10s 163us/step - loss: 0.1000 - acc: 0.9560 - val_loss: 0.0204 - val_acc: 0.9940
    Epoch 18/20

    Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.
    60000/60000 [==============================] - 10s 161us/step - loss: 0.0981 - acc: 0.9562 - val_loss: 0.0202 - val_acc: 0.9946
    Epoch 19/20

    Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.
    60000/60000 [==============================] - 10s 162us/step - loss: 0.0962 - acc: 0.9563 - val_loss: 0.0221 - val_acc: 0.9934
    Epoch 20/20

    Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.
    60000/60000 [==============================] - 10s 164us/step - loss: 0.0956 - acc: 0.9566 - val_loss: 0.0212 - val_acc: 0.9941

    <keras.callbacks.History at 0x7fa52cc419e8>

Test accuracy :

    [0.02122559410687536, 0.9941]

    99.41 %

Method used :

    I have used a series of block(16 channels with 3x3) and then inserted a 
    1x1 kernel to filter important features and then a maxpooling for increasing the receptive field then i have used global average pooling at the end for efficiency .
    
